# for available projector models see irt2m.models.PROJECTORS
model: multi context complex joint
model_kwargs:
  embedding_dim: 500
  loss: cross entropy
  regularizer: LPRegularizer
  regularizer_kwargs:
    p: 2.0
    weight: 0.1
    normalize: true


# which kgc evaluations to run while training
evaluations:
  - kgc/inductive     # open world kgc with projections (irt2.open_kgc_val*)
  - irt2/inductive    # official irt2 evaluation


# for available datasets see irt2m.data.PROJECTOR_DATASETS
#   and their respective __init__ methods for *ds_kwargs
#
# contexts_per_sample is the amount of text contexts
# subbatch_size is batch_size if None
#
# provide the maximum expected contexts_per_sample here to allow
# for batch-size recalculation if --contexts-per-sample is overwritten
# via cli!
module:
  train_ds: vertex entity ringbuffer
  train_ds_kwargs:
    seed: 5012022
    contexts_per_sample: 10  # per batch
    max_contexts_per_sample: 100
  train_loader_kwargs:
    shuffle: true
    batch_size: 1
    subbatch_size: 5

  valid_ds: mention ringbuffer
  valid_ds_kwargs:
    seed: 5012022
    contexts_per_sample: 10
    max_contexts_per_sample: 100
  valid_loader_kwargs:
    shuffle: false
    batch_size: 30

# for available optimizers see irt2m.models.OPTIMIZERS
optimizer: adam
optimizer_kwargs:
  lr: 5.0e-05
  weight_decay: 0.01

early_stopping: true
early_stopping_kwargs:
  min_delta: 0.001
  patience: 5
  mode: max
  monitor: irt2/inductive/all micro hits@10
