# Configuration for projector training

# irt2m

# tested: bert base models (see transformers lib)
encoder: bert-base-cased

# available format keys:
#   prefix: short prefix based on the configuration (see train._build_prefix)
#   dataset: the (normalised) upstream IRT2.name property (e.g. irt2-cde-l)
#   date: start datetime formatted as %Y-%m-%d_%H-%M-%S
#   encoder: as defined
#   projector: as defined
out: data/models/{prefix}/{date}


evaluations_kwargs:
  irt2:
    batch_size: 100

# there are three modes:
#    probe:
#      - do the "fast_dev_run" for sanity checks:
#        one batch per train and validation
#      - logger is disabled
#      - no data is written to disk
#    limited:
#      - adjust numbers in train.py:
#        - run some batches per validation and train
#        - evaluate N triples in kgc evaluation and irt2 evaluation
#        - run for M epochs
#      - logger is disabled
#      - no data is written to disk
#      - profiling is active
#    full
#      - run all batches
#      - logging to wandb
#      - all data is written to disk

# mode: probe
# mode: limited
mode: full

# pytorch lightning

# kgc/inductive/hits@10
# irt2/inductive/all micro hits@10

checkpoint:
  every_n_epochs: 3
  mode: max
  monitor: irt2/inductive/all micro hits@10
  save_last: true
  save_top_k: 1

scheduler: constant
scheduler_kwargs: {}

trainer:
  gpus: 1
  max_epochs: 800
  check_val_every_n_epoch: 3


wandb:
  project: irt2-projector
