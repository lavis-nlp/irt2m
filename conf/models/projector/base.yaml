# Configuration for projector training

# irt2m

# tested: bert base models (see transformers lib)
encoder: bert-base-cased

# for available projector models see irt2m.models.PROJECTORS
model: single context affine projector

# available format keys:
#   prefix: short prefix based on the configuration (see train._build_prefix)
#   dataset: the (normalised) upstream IRT2.name property (e.g. irt2-cde-l)
#   date: start datetime formatted as %Y-%m-%d_%H-%M-%S
#   encoder: as defined
#   projector: as defined
out: data/models/{prefix}/{date}

# which kgc evaluations to run while training
evaluations:
  - kgc/train         # closed world kgc with original embeddings (run once before training)
  - kgc/transductive  # closed world kgc with projections (irt2.closed_triples)
  - kgc/inductive     # open world kgc with projections (irt2.open_kgc_val*)
  # - kgc/test          # currently disabled


# irt2m.data

# for available datasets see irt2m.data.PROJECTOR_DATASETS
#   and their respective __init__ methods for *ds_kwargs
module:
  train_ds: vertex ringbuffer
  train_ds_kwargs:
    seed: 5012022
    contexts_per_sample: 10  # per batch
    max_contexts_per_sample: 1000
  train_loader_kwargs:
    batch_size: 1
    subbatch_size: 10

  valid_ds: mention ringbuffer
  valid_ds_kwargs:
    seed: 5012022
    contexts_per_sample: 30  # per batch
    max_contexts_per_sample: 1000
  valid_loader_kwargs:
    batch_size: 1


# pytorch

# for available optimizers see irt2m.models.OPTIMIZERS
optimizer: adam
optimizer_kwargs:
  lr: 1.0e-05

# pytorch lightning

checkpoint:
  every_n_epochs: 1
  mode: min
  monitor: training/loss  # see models.Metric
  save_last: true
  save_top_k: 1

early_stopping_args:
  mode: max
  monitor: kgc/inductive/hits@10

scheduler: constant
scheduler_kwargs: {}

trainer:
  gpus: 1

  # 12886 steps = 1 epoch for cde-l
  # max_steps: 500_000
  max_epochs: 10

  # limit_train_batches: 100
  # limit_val_batches: 100

  # this is the "debug" flag
  fast_dev_run: false

wandb:
  project: irt2-projector
